<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Gregor, The Coding Cat | A personal blog about CS stuff</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Gregor, The Coding Cat" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A personal blog about CS stuff" />
<meta property="og:description" content="A personal blog about CS stuff" />
<link rel="canonical" href="http://localhost:4000/parallel_computing/notes_3/" />
<meta property="og:url" content="http://localhost:4000/parallel_computing/notes_3/" />
<meta property="og:site_name" content="Gregor, The Coding Cat" />
<script type="application/ld+json">
{"description":"A personal blog about CS stuff","@type":"WebPage","url":"http://localhost:4000/parallel_computing/notes_3/","headline":"Gregor, The Coding Cat","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <header>

      <div class="container">

        <h1>Gregor, The Coding Cat</h1>
        <h2>A personal blog about CS stuff</h2>

      </div>
      
    </header>


    <div class="container">
      <section id="main_content">
        <h2 id="hello-world-intro-to-cuda-programming">Hello World: Intro to CUDA programming</h2>

<p>Well, to get started let’s do a basic hello world program in CUDA. Not an actual “print ‘helloWorld’”, but an add two numbers example. Should be basic enough and give you the basic of how to write a CUDA program.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// add.cu</span>
<span class="c1">// Add two number example</span>

<span class="cp">#include &lt;iostream&gt;
</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">add</span><span class="p">(</span> <span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">pRes</span> <span class="p">)</span>
<span class="p">{</span>
    <span class="o">*</span><span class="n">pRes</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
    <span class="c1">// Create some pointers for GPU memory and cpu memory</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">d_res</span><span class="p">;</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">h_res</span><span class="p">;</span>

    <span class="c1">// Allocate some memory in the GPU, just one int for our result </span>
    <span class="n">cudaMalloc</span><span class="p">(</span> <span class="p">(</span> <span class="kt">void</span><span class="o">**</span> <span class="p">)</span><span class="o">&amp;</span><span class="n">d_res</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span> <span class="kt">int</span> <span class="p">)</span> <span class="p">);</span>

    <span class="c1">// Also, allocate memory for the result, to get it back from our GPU</span>
    <span class="n">h_res</span> <span class="o">=</span> <span class="p">(</span> <span class="kt">int</span><span class="o">*</span> <span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="k">sizeof</span><span class="p">(</span> <span class="kt">int</span> <span class="p">)</span> <span class="p">);</span>

    <span class="c1">// Call our add "function" ( function, but fancy named "kernel" )</span>
    <span class="n">add</span><span class="o">&lt;&lt;&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&gt;&gt;&gt;</span><span class="p">(</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">d_res</span> <span class="p">);</span>

    <span class="c1">// After the computation, get back the result</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span> <span class="n">h_res</span><span class="p">,</span> <span class="n">d_res</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span> <span class="kt">int</span> <span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span> <span class="p">);</span>

    <span class="c1">// Print our result</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"2 + 7 = "</span> <span class="o">&lt;&lt;</span> <span class="o">*</span><span class="n">h_res</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>

    <span class="c1">// Free the memory on the host</span>
    <span class="n">free</span><span class="p">(</span> <span class="n">h_res</span> <span class="p">);</span>
    <span class="c1">// Free the memory on the device ( GPU )</span>
    <span class="n">cudaFree</span><span class="p">(</span> <span class="n">d_res</span> <span class="p">);</span>
<span class="p">}</span>

</code></pre></div></div>

<p>Let’s analyze the parts of the code. Let’s start by the main method.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kt">int</span> <span class="o">*</span><span class="n">d_res</span><span class="p">;</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">h_res</span><span class="p">;</span>
</code></pre></div></div>

<p>In this part we just create two pointers. As we will see later, they will point to different kinds of memory :D. The <em>h_res</em> pointer will point to memory in our host computer ( that’s why the ‘h_’ in the variable name ), and the other pointer, <em>d_res</em>, will point to memory in our device ( our GPU, that’s why the ‘d_’ ), which is a big fat chunk of global memory in the GPU, around 2GB, depending of your GPU ( mine, a GTX 750 Ti has just 2GB :’( );</p>

<p>It’s a good practice to keep track of the pointers we use with this ‘d_’ and ‘h_’ notation, as will make it clear to which kind of data they are pointing to. The most common error you could run into is trying to dereference the memory pointed by a device pointer, which will make our program crash spectacularly xD ( well, the most annoying bugs to me are the ones that freeze your computer, but this is also really annoying, like forgetting a “;” ).</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// Allocate some memory in the GPU, just one int for our result </span>
    <span class="n">cudaMalloc</span><span class="p">(</span> <span class="p">(</span> <span class="kt">void</span><span class="o">**</span> <span class="p">)</span><span class="o">&amp;</span><span class="n">d_res</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span> <span class="kt">int</span> <span class="p">)</span> <span class="p">);</span>
</code></pre></div></div>

<p>This part is a call to the CUDA API, specifically the ‘cudaMalloc’ function, which is similar to the old good malloc in host code. Like malloc, is in charge or reserving memory, but in this case in the device ( GPU ) and “returns” ( it writes the direction to ) a pointer to that reserved block of memory.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// Also, allocate memory for the result, to get it back from our GPU</span>
    <span class="n">h_res</span> <span class="o">=</span> <span class="p">(</span> <span class="kt">int</span><span class="o">*</span> <span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="k">sizeof</span><span class="p">(</span> <span class="kt">int</span> <span class="p">)</span> <span class="p">);</span>
</code></pre></div></div>

<p>This is the old good malloc, which just reserves memory on the host, and returns a pointer to the block reserved.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// Call our add "function" ( function, but fancy named "kernel" )</span>
    <span class="n">add</span><span class="o">&lt;&lt;&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">&gt;&gt;&gt;</span><span class="p">(</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">d_res</span> <span class="p">);</span>
</code></pre></div></div>

<p>This is the fun part. Here we request the runtime to execute our “kernel”, called “add”, in the GPU. You see there are some extra stuff that make this call different to a simple call to a c++ host function, which are the “«&lt;”, “»&gt;” brackets. This is a special qualifier that tells the runtime to execute our kernel.</p>

<p>You should be wondering what are those parameters inside “«&lt; »&gt;”. Well, if you recall a GPU has MANY cores, which mean we can execute our device code into MANY cores. The parameters passed to that qualifier are the way of controlling how many of those core are going to run our device code and also how they should distribute the work. We will see more on how the work is distributed in the notes that deal with the GPU Architecture, but will start manipulating these parameters and explain the basics of how to distribute work in the GPU in the next notes.</p>

<p>In our case, just 1 “thread” is requested. The GPU will use its magic to know which core should run this thread, so we don’t have to worry about that. It’s kind of like a scheduler in an Operating System.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">add</span><span class="p">(</span> <span class="kt">int</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">pRes</span> <span class="p">)</span>
<span class="p">{</span>
    <span class="o">*</span><span class="n">pRes</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This part is actually out of main, but it’s better to take a look at it now. This is the code that will run in the device. You can <del>see</del> it by the “<strong>global</strong>” keyword. This qualifier tells the compiler to generate device code instead of host code. Here, the compiler sees the bifurcation and does what I mentioned. The CUDA compiler is actually a wrapper on top of the host compiler ( g++, for example ), which transfers the work to the host compiler when needed ( host code ) and transfering it to the CUDA compiler when needed ( device code ).</p>

<p>Appart from the return type, which is void, and the fancy “<strong>global</strong>” keyword, everything is the same as in any c++ function. It has input parameters, which in this case are two integers <em>a</em> and <em>b</em>, and a pointer ( to device memory ) <em>pRes</em>. In the device code, we just use the variables and pointers the same way we would do in host code. There is no funky method like “cudaUseMemory” or something like that, although there are some special functions that deal with special cases, like “atomicAdd”, which we will study later.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// After the computation, get back the result</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span> <span class="n">h_res</span><span class="p">,</span> <span class="n">d_res</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span> <span class="kt">int</span> <span class="p">),</span> <span class="n">cudaMemcpyDeviceToHost</span> <span class="p">);</span>
</code></pre></div></div>

<p>Ok, so far we have requested the device to execute our add kernel, but the result is still in GPU world. We have to bring back that data, as we can dereference it in host code. 
Here we use another CUDA function, the “cudaMemcpy” function. It acts as a regular memcpy in host code, but the key difference is that it can deal with not only device to device copies, but also with host to device and device to host copies. In our case we want the data back from device memory to host memory, so the direction of copy will be deviceToHost ( like the last parameter in our function call, cudaMemcpyDeviceToHost ).</p>

<p>The parameters that we pass are :</p>

<ol>
  <li>destination pointer, which is to where we want to copy the memory to. In our case is the host pointer <em>h_res</em></li>
  <li>source pointer, which is from where we are going to copy the memory. It’s our device pointer <em>d_res</em> in this case.</li>
  <li>amount of memory to copy, which is the size of an int in our case as we are just using a single integer.</li>
  <li>the direction of the copy, which is cudaMemcpyDeviceToHost in our case.</li>
</ol>

<p>Once completed, we will have the result in the block of memory pointed by <em>h_res</em>.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// Print our result</span>
    <span class="n">std</span><span class="o">::</span><span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"2 + 7 = "</span> <span class="o">&lt;&lt;</span> <span class="o">*</span><span class="n">h_res</span> <span class="o">&lt;&lt;</span> <span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
</code></pre></div></div>

<p>Here we just print the result. Nothing fancy.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// Free the memory on the host</span>
    <span class="n">free</span><span class="p">(</span> <span class="n">h_res</span> <span class="p">);</span>
    <span class="c1">// Free the memory on the device ( GPU )</span>
    <span class="n">cudaFree</span><span class="p">(</span> <span class="n">d_res</span> <span class="p">);</span>
</code></pre></div></div>

<p>Finally, once the data has served its purpose, we have to free it. We free the host memory by the call to the usual “free” in host code. We also have to free the memory in the GPU, which is done by the CUDA function “cudaFree”. Of course, instead of passing a host pointer, we pass a device pointer.</p>

<p>When the program terminates all the memory used by the GPU is actually released, but we should call cudaFree when the memory is not longer necessary. As an example, imagine that we are dealing with block of memory in the GPU that are related to matrices. Once a matrix is no longer used, we should free its memory, as it could accumulate. Bad things could happen if we run out of memory. Memory leaks are a common issue in host c/c++ code, and they are also a issue in device code.</p>

<p>To compile our program we just have to call the “nvcc” compiler, like this :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvcc -o add.out add.cu
</code></pre></div></div>

<p>This will compile our <em>add.cu</em> file and generate the executable <em>add.out</em>. Let’s just run it by calling :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./add.out
</code></pre></div></div>

<p>like any other regular executable. Of course, we should get this output :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    2 + 7 = 9
</code></pre></div></div>

<p>If you get anything different from 9 it could mean you are not memcopying correctly to the correct pointer. It can also mean that you have no CUDA capable GPU xD, as I did in my laptop which doesn’t. You can actually have the cuda toolkit installed and compile code, but if no GPU is present you wont gen any useful result.</p>

<p>This takes us to talk a little about the device. We have some control over it, but how do we know the resources and limitations of our GPU, or if we have any GPU at all.</p>

<p>We will discuss this in the next section, as this section’s notes are getting quite large. See you there.</p>

      </section>
    </div>

  </body>
</html>

<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> 
    </script>
<!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Gregor, The Coding Cat | A personal blog about CS stuff</title>
<meta name="generator" content="Jekyll v3.7.3" />
<meta property="og:title" content="Gregor, The Coding Cat" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A personal blog about CS stuff" />
<meta property="og:description" content="A personal blog about CS stuff" />
<link rel="canonical" href="http://localhost:4000/numerical_linear_algebra/notes_22/" />
<meta property="og:url" content="http://localhost:4000/numerical_linear_algebra/notes_22/" />
<meta property="og:site_name" content="Gregor, The Coding Cat" />
<script type="application/ld+json">
{"description":"A personal blog about CS stuff","@type":"WebPage","url":"http://localhost:4000/numerical_linear_algebra/notes_22/","headline":"Gregor, The Coding Cat","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <header>

      <div class="container">

        <h1>Gregor, The Coding Cat</h1>
        <h2>A personal blog about CS stuff</h2>

      </div>
      
    </header>


    <div class="container">
      <section id="main_content">
        <h1 id="conjugate-gradient-method">Conjugate Gradient method</h1>

<h3 id="iteration-step">Iteration step</h3>

<p>The iteration for the conjugate gradient is quite similar to the basic gradient method. The only difference are the direction vectors used 
to update the solution vector in the iteration step. Instead of the residual, we compute a conjugate direction based on the residual.</p>

<p>The iteration is as follows :</p>

<p>\[
	\textbf{ Conjugate Gradient iteration }\\
	\alpha_{k} = \frac{r_{k}^{T}r_{k}}{p_{k}^{T}Ap_{k}}\\
	x_{k+1} = x_{k} + \alpha_{k} p_{k} \\
	r_{k+1} = r_{k} - \alpha_{k} A p_{k} \\
	\beta_{k} = \frac{r_{k+1}^{T}r_{k+1}}{r_{k}^{T}r_{k}} \\
	p_{k+1} = r_{k+1} + \beta_{k} p_{k}
\]</p>

      </section>
    </div>

  </body>
</html>
